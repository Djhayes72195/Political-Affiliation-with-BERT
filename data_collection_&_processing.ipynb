{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "client = tweepy.Client('<BearerToken>')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook was used to mine and process the data that we used to train our BERT classifier.  The starting point of our data collection is a dataset we found on Kaggle https://www.kaggle.com/datasets/mrmorj/us-politicians-twitter-dataset?resource=download. This dataset is the source of the Twitter IDs used for text data collection, as well as our ground truth labels. I have done my best to make this project as replicable as possible. For my own privacy, however, I have not included the Twitter API Bearer Token that would be required to interact with the Twitter API.  As such, the code here can't be used to fetch more tweets or filter by account activity unless another Bearer token were provided. I will do my best to describe what happened during the gaps where the project is not replicable with the code presented here.  With the exception of these gaps, the code could be ran from top to bottom to reproduce the data collection and processing aspects of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_third_party(pol_data):\n",
    "    \"\"\"This code sets party to \"Democrat\" (or \"Republican\") if \"Democrat\" (or \"Republican\") is contained in the name of the party in the original kaggle dataset.\"\"\"\n",
    "    pol_data = pd.read_csv('Data\\\\Original_kaggle_dataset.csv')\n",
    "    df = pol_data[['Account_ID', 'Name', 'Twitter_username', 'Political_party']]\n",
    "    df = df.loc[df['Political_party'].str.contains('Republican') | df['Political_party'].str.contains('Democrat')]\n",
    "    df = df.drop_duplicates(subset=['Account_ID'])\n",
    "    df.loc[df['Political_party'].str.contains('Republican'), 'Political_party'] = 'Republican Party'\n",
    "    df.loc[df['Political_party'].str.contains('Democrat'), 'Political_party'] = 'Democratic Party'\n",
    "    df.to_csv('Data\\\\Dem_Rep_only.csv', index=False)\n",
    "\n",
    "data = pd.read_csv('Data\\\\Original_kaggle_dataset.csv')\n",
    "drop_third_party(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_active_and_valid(user_id: str):\n",
    "    \"\"\"This functions returns False if the user has not been active since October 1 of 2022,\n",
    "    True otherwise.  It should also return false for bad (not numeric) IDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check = client.get_users_tweets(id=user_id,max_results=10, start_time='2022-10-01T00:00:00Z')\n",
    "    except:\n",
    "        return False\n",
    "    return (not check.data is None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell won't be runable without a Bearer Token.  Its purpose is to filter our dataset down so as to only include accounts that have been active since Oct 1, 2022.  As it turns out, it is quite common for a politician to have one or more unactive accounts they have since abandonded.  Maybe they forgot their password.  When we ran this function mid November of 2022 it filtered out nearly half of our rows, producing the dataset we called \"cleanest_politicians.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_data_cleaner_by_last_active(df):\n",
    "    \"\"\"This function will take a data frame representing twitter\n",
    "    users, and will take out anyone that has not been active\n",
    "    since Oct 1 2022.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    ID_list = list(df['Account_ID'])\n",
    "    ID_list_str = [str(x) for x in ID_list]\n",
    "    df['Active'] = [check_if_active_and_valid(x) for x in ID_list_str]\n",
    "    clean_df = df[df['Active'] == True]\n",
    "    clean_df = clean_df.drop(['Active'], axis=1)\n",
    "    return clean_df\n",
    "\n",
    "data = pd.read_csv('Data\\\\Dem_Rep_only.csv')\n",
    "clean = pol_data_cleaner_by_last_active(data)\n",
    "clean.to_csv('Data\\\\cleanest_politicians.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function is the one we used to get 1000 tweets per Account ID.  Initially, we sorted our data in a collection of csv files (one per politician).  It made it easier to manually inspect the data we were getting.  The data for each politician is stored in this project under Data\\politician_csvs\\{ID}-{Name}, but we combined the data into one csv called \"test_party_IDs.csv\" for training/testing the model itself.  Note: sometimes the Twitter API failed to get all 1000 tweets from a particular politician.  Those politicians were removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paged_tweets(user_id):\n",
    "    \"\"\"Function for gathering raw tweets for a list of user id and saving them to csv.\"\"\"\n",
    "    df = pd.DataFrame(columns=['TweetID', 'UserID', 'Text'])\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, exclude='retweets',\n",
    "                                  max_results=100).flatten(limit=1000):\n",
    "        # remove tab\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        #append raw tweet\n",
    "        df.loc[len(df.index)] = [tweet.id, user_id, text]\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Data\\\\cleanest_politicians.csv')\n",
    "ID_list = list(df['Account_ID'])\n",
    "name_list = list(df['Name'])\n",
    "\n",
    "   \n",
    "for i in range(0, len(ID_list)):\n",
    "    file = Path('Data\\\\politician_csvs\\\\{}-{}.csv'.format((ID_list[i]), name_list[i]))\n",
    "    if file.exists():\n",
    "        print(\"passed\")\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        df = get_paged_tweets(ID_list[i])\n",
    "        df.to_csv('Data\\\\politician_csvs\\\\{}-{}.csv'.format((ID_list[i]), name_list[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell filters out politicians that the Twitter API couldn't fetch all 1000 tweets for.  All politicians that we could get 1000 tweets from are put in other folder: \"1000_only\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter csvs such that only pol with X or more tweets included\n",
    "\n",
    "def make_pol_csvs_min_only(csv_path: str, min_tweets: int):\n",
    "    df = pd.read_csv(\"Data\\\\politician_csvs\\\\{}\".format(csv_path), lineterminator='\\n')\n",
    "    if len(list(df['TweetID'])) >= min_tweets:\n",
    "        print(len(list(df['TweetID'])))\n",
    "        file = Path(\"Data\\\\1000_only\\\\{}\".format(csv_path))\n",
    "        if file.exists():\n",
    "            pass\n",
    "        else:\n",
    "            os.rename(\"Data\\\\politician_csvs\\\\{}\".format(csv_path), \"Data\\\\1000_only\\\\{}\".format(csv_path))\n",
    "\n",
    "pol_csvs = os.listdir(\"Data\\\\politician_csvs\")\n",
    "for i in range(0, len(pol_csvs)):\n",
    "    make_pol_csvs_min_only(pol_csvs[i], 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we combine all Tweets into one csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\3753647276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcombine_tweet_csvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data\\\\1000_only\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9828\\3753647276.py\u001b[0m in \u001b[0;36mcombine_tweet_csvs\u001b[1;34m(csv_dir)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mfull_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpol_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}\\\\{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mtemp_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UserID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mfull_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 )\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m                 \u001b[0mis_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1730\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    858\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Pork Chop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mbyte\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;31m# undecoded input that is kept between calls to decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def combine_tweet_csvs(csv_dir):\n",
    "    pol_list = os.listdir(csv_dir)\n",
    "    full_df = pd.DataFrame()\n",
    "    for pol in pol_list:\n",
    "        temp_df = pd.read_csv(\"{}\\\\{}\".format(csv_dir, pol), lineterminator='\\n', index_col=0)\n",
    "        temp_df['UserID'] = pol.split(\"-\")[0]\n",
    "        full_df = pd.concat([temp_df, full_df])\n",
    "    full_df.to_csv('Data\\\\{}'.format('full_csv_test.csv'), index=False)\n",
    "\n",
    "\n",
    "combine_tweet_csvs(\"Data\\\\1000_only\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to add ground truth labels and the data will be ready for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745000, 4)\n"
     ]
    }
   ],
   "source": [
    "def add_labels(pre_data_path):\n",
    "    pre_df = pd.read_csv(pre_data_path)\n",
    "    label_source = pd.read_csv('Data\\\\Dem_Rep_only.csv')\n",
    "    IDs = list(label_source['Account_ID'])\n",
    "    labels = list(label_source['Political_party'])\n",
    "    ID_label_dict = {IDs[i]: labels[i] for i in range(len(IDs))}\n",
    "    pre_df['Party'] = np.nan\n",
    "    for id in IDs:\n",
    "        try:\n",
    "            int_id = int(id)\n",
    "            pre_df.loc[pre_df[\"UserID\"] == int(id), 'Party'] = ID_label_dict[id]\n",
    "        except:\n",
    "            pass\n",
    "    print(pre_df.shape)\n",
    "    pre_df.to_csv('Data\\\\test_full_w_party.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "add_labels('Data\\\\full_csv_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_by_userID_list(users_id):\n",
    "    dictionary = {}\n",
    "    for i in users_id:\n",
    "        tweetlist =[]\n",
    "        usertweet = client.get_users_tweets(id=i,max_results=100)\n",
    "        for tweet in usertweet.data:\n",
    "            tweetlist.append(tweet.text)\n",
    "        dictionary[i] = tweetlist\n",
    "    return(dictionary)\n",
    "\n",
    "print(get_tweets_by_userID_list(['487297085']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets by one Obama account\n",
    "def check_if_active_and_valid(user_id: str):\n",
    "    \"\"\"This functions returns False if the user has not been active since October 1 of 2022,\n",
    "    True otherwise.  It should also return false for bad (not numeric) IDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check = client.get_users_tweets(id=user_id,max_results=10, start_time='2022-10-01T00:00:00Z')\n",
    "    except:\n",
    "        return False\n",
    "    return (not check.data is None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_data_cleaner_by_last_active(df):\n",
    "    \"\"\"This function will take a data frame representing twitter\n",
    "    users, and will take out anyone that has not been active\n",
    "    since Oct 1 2022.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    ID_list = list(df['Account_ID'])\n",
    "    ID_list_str = [str(x) for x in ID_list]\n",
    "    df['Active'] = [check_if_active_and_valid(x) for x in ID_list_str]\n",
    "    clean_df = df[df['Active'] == True]\n",
    "    clean_df = clean_df.drop(['Active'], axis=1)\n",
    "    return clean_df\n",
    "\n",
    "data = pd.read_csv('cleaner_politicians.csv')\n",
    "clean = pol_data_cleaner_by_last_active(data)\n",
    "print(clean)\n",
    "clean.to_csv('cleanest_politicians.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from cleantext import clean\n",
    "def twitter(users_id):\n",
    "    \"\"\"Script for gathering raw tweets for a list of user id\"\"\"\n",
    "    dictionary = {}\n",
    "    for i in users_id:\n",
    "        tweetlist =[]\n",
    "        usertweet = client.get_users_tweets(id=i,max_results=10)\n",
    "        for tweet in usertweet.data:\n",
    "            #get rid of emoji\n",
    "            cleantext = clean(str(tweet.text))\n",
    "            # remove tab\n",
    "            text = cleantext.replace('\\n',' ')\n",
    "            # get rid of url/links\n",
    "            rawtwt = re.sub(r'https?:\\/\\/\\S*', '', text, flags=re.MULTILINE)\n",
    "            #append raw tweet\n",
    "            tweetlist.append(rawtwt)\n",
    "        dictionary[i] = tweetlist\n",
    "    return(dictionary)\n",
    "\n",
    "print(twitter(['216776631']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paginator testing\n",
    "\n",
    "counter = 0\n",
    "for tweet in tweepy.Paginator(client.get_users_tweets, \"216776631\",\n",
    "                                max_results=10).flatten(limit=10):\n",
    "    print(tweet.id)\n",
    "    counter += 1\n",
    "\n",
    "print(counter)\n",
    "\n",
    "#It works, we can get a lot more tweets per user this way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tweets with pagination, put them into a csv\n",
    "\n",
    "import re\n",
    "from cleantext import clean\n",
    "def get_paged_tweets(user_id):\n",
    "    \"\"\"Script for gathering raw tweets for a list of user id and saving them to csv\"\"\"\n",
    "    df = pd.DataFrame(columns=['TweetID', 'UserID', 'Text'])\n",
    "    tweetlist =[]\n",
    "        # usertweet_page = tweepy.Paginator(client.get_users_tweets, i,\n",
    "                                # max_results=10).flatten(limit=10)\n",
    "        # print(usertweet_page.id)\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, exclude='retweets',\n",
    "                                  max_results=100).flatten(limit=1000):\n",
    "        #     #get rid of emoji\n",
    "        # tweet_text = tweet.text.lower()\n",
    "        # clean_text = clean(str(tweet_text))\n",
    "        #     # remove tab\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        # print(text)\n",
    "        #     # get rid of url/links\n",
    "        # rawtwt = re.sub(r'https?:\\/\\/\\S*', '', text, flags=re.MULTILINE)\n",
    "        #     #append raw tweet\n",
    "        df.loc[len(df.index)] = [tweet.id, user_id, text]\n",
    "    return df\n",
    "\n",
    "print(get_paged_tweets('487297085'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct csvs for each politician\n",
    "from pathlib import Path\n",
    "\n",
    "def fetch_tweets(ID_list, name_list):\n",
    "    \n",
    "    # df = pd.read_csv('cleanest_politicians.csv')\n",
    "    # ID_list = list(df['Account_ID'])\n",
    "    # name_list = list(df['Name'])\n",
    "\n",
    "        \n",
    "    for i in range(0, len(ID_list)):\n",
    "        file = Path('/Users/dustinhayes/Desktop/STAT766Final/fixed_pol_csvs/{}-{}.csv'.format((ID_list[i]), name_list[i]))\n",
    "        print('getting {}'.format(name_list[i]))\n",
    "        if file.exists():\n",
    "            pass\n",
    "        else:\n",
    "            df = get_paged_tweets(ID_list[i])\n",
    "            df.to_csv('/Users/dustinhayes/Desktop/STAT766Final/fixed_pol_csvs/{}-{}.csv'.format((ID_list[i]), name_list[i]), index=False)\n",
    "\n",
    "fetch_tweets(['80013913', '256074273', '487297085', '854715071116849157', '233737858'], ['Ricardo Rossello', 'Roland Gtierrez', 'Ron DeSantis', 'Ron Estes', 'Ron John'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter csvs such that only pol with X or more tweets included\n",
    "\n",
    "def make_pol_csvs_min_only(csv_path: str, min_tweets: int):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if len(list(df['TweetID'])) >= min_tweets:\n",
    "        df.to_csv('{}\\\\{}'.format(min_tweets))\n",
    "        print(csv_path)\n",
    "\n",
    "os.chdir('/Users/dustinhayes/Desktop/STAT766Final/politician_csvs')\n",
    "pol_csvs = os.listdir()\n",
    "for i in range(0, len(pol_csvs[0:5])):\n",
    "    make_pol_csvs_min_only(pol_csvs[i], 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19144ae99ecdd7de40b02483c8dd0c3f376ecc4a896bed5b8b3dc837c5c7c700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
