{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "client = tweepy.Client('<BearerToken>')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook was used to mine and process the data that we used to train our BERT classifier.  The starting point of our data collection is a dataset we found on Kaggle https://www.kaggle.com/datasets/mrmorj/us-politicians-twitter-dataset?resource=download. This dataset is the source of the Twitter IDs used for text data collection, as well as our ground truth labels. I have done my best to make this project as replicable as possible. For my own privacy, however, I have not included the Twitter API Bearer Token that would be required to interact with the Twitter API.  As such, the code here can't be used to fetch more tweets or filter by account activity unless another Bearer token were provided. I will do my best to describe what happened during the gaps where the project is not replicable with the code presented here.  With the exception of these gaps, the code could be ran from top to bottom to reproduce the data collection and processing aspects of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_third_party(pol_data):\n",
    "    \"\"\"This code sets party to \"Democrat\" (or \"Republican\") if \"Democrat\" (or \"Republican\") is contained in the name of the party in the original kaggle dataset.\"\"\"\n",
    "    pol_data = pd.read_csv('Data\\\\Original_kaggle_dataset.csv')\n",
    "    df = pol_data[['Account_ID', 'Name', 'Twitter_username', 'Political_party']]\n",
    "    df = df.loc[df['Political_party'].str.contains('Republican') | df['Political_party'].str.contains('Democrat')]\n",
    "    df = df.drop_duplicates(subset=['Account_ID'])\n",
    "    df.loc[df['Political_party'].str.contains('Republican'), 'Political_party'] = 'Republican Party'\n",
    "    df.loc[df['Political_party'].str.contains('Democrat'), 'Political_party'] = 'Democratic Party'\n",
    "    df.to_csv('Data\\\\Dem_Rep_only.csv', index=False)\n",
    "\n",
    "data = pd.read_csv('Data\\\\Original_kaggle_dataset.csv')\n",
    "drop_third_party(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_active_and_valid(user_id: str):\n",
    "    \"\"\"This functions returns False if the user has not been active since October 1 of 2022,\n",
    "    True otherwise.  It should also return false for bad (not numeric) IDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check = client.get_users_tweets(id=user_id,max_results=10, start_time='2022-10-01T00:00:00Z')\n",
    "    except:\n",
    "        return False\n",
    "    return (not check.data is None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell won't be runable without a Bearer Token.  Its purpose is to filter our dataset down so as to only include accounts that have been active since Oct 1, 2022.  As it turns out, it is quite common for a politician to have abandonded one or more twitter accounts.  Maybe they forgot their password.  When we ran this function mid November of 2022 it filtered out nearly half of our rows, producing the dataset we called \"cleanest_politicians.csv\".  Note that this function is in no way certain to return the same thing from one day to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_data_cleaner_by_last_active(df):\n",
    "    \"\"\"This function will take a data frame representing twitter\n",
    "    users, and will take out anyone that has not been active\n",
    "    since Oct 1 2022.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    ID_list = list(df['Account_ID'])\n",
    "    ID_list_str = [str(x) for x in ID_list]\n",
    "    df['Active'] = [check_if_active_and_valid(x) for x in ID_list_str]\n",
    "    clean_df = df[df['Active'] == True]\n",
    "    clean_df = clean_df.drop(['Active'], axis=1)\n",
    "    return clean_df\n",
    "\n",
    "data = pd.read_csv('Data\\\\Dem_Rep_only.csv')\n",
    "clean = pol_data_cleaner_by_last_active(data)\n",
    "clean.to_csv('Data\\\\cleanest_politicians.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next function is the one we used to get 1000 tweets per Account ID.  Initially, we sorted our data in a collection of csv files (one per politician).  It made it easier to manually inspect the data we were getting.  The data for each politician is stored in this project under Data\\politician_csvs\\{ID}-{Name}, but we combined the data into one csv called \"test_party_IDs.csv\" for training/testing the model itself.  Note: sometimes the Twitter API failed to get all 1000 tweets from a particular politician.  Those politicians were removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paged_tweets(user_id):\n",
    "    \"\"\"Function for gathering raw tweets for a list of user id and saving them to csv.\"\"\"\n",
    "    df = pd.DataFrame(columns=['TweetID', 'UserID', 'Text'])\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, exclude='retweets',\n",
    "                                  max_results=100).flatten(limit=1000):\n",
    "        # remove tab\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        #append raw tweet\n",
    "        df.loc[len(df.index)] = [tweet.id, user_id, text]\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv('Data\\\\cleanest_politicians.csv')\n",
    "ID_list = list(df['Account_ID'])\n",
    "name_list = list(df['Name'])\n",
    "\n",
    "   \n",
    "for i in range(0, len(ID_list)):\n",
    "    file = Path('Data\\\\politician_csvs\\\\{}-{}.csv'.format((ID_list[i]), name_list[i]))\n",
    "    if file.exists():\n",
    "        print(\"passed\")\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        df = get_paged_tweets(ID_list[i])\n",
    "        df.to_csv('Data\\\\politician_csvs\\\\{}-{}.csv'.format((ID_list[i]), name_list[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell filters out politicians that the Twitter API couldn't fetch all 1000 tweets for.  All politicians that we could get 1000 tweets from are put in other folder: \"1000_only\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter csvs such that only pol with X or more tweets included\n",
    "\n",
    "def make_pol_csvs_min_only(csv_path: str, min_tweets: int):\n",
    "    df = pd.read_csv(\"Data\\\\politician_csvs\\\\{}\".format(csv_path), lineterminator='\\n')\n",
    "    if len(list(df['TweetID'])) >= min_tweets:\n",
    "        print(len(list(df['TweetID'])))\n",
    "        file = Path(\"Data\\\\1000_only\\\\{}\".format(csv_path))\n",
    "        if file.exists():\n",
    "            pass\n",
    "        else:\n",
    "            os.rename(\"Data\\\\politician_csvs\\\\{}\".format(csv_path), \"Data\\\\1000_only\\\\{}\".format(csv_path))\n",
    "\n",
    "pol_csvs = os.listdir(\"Data\\\\politician_csvs\")\n",
    "for i in range(0, len(pol_csvs)):\n",
    "    make_pol_csvs_min_only(pol_csvs[i], 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we combine all Tweets into one csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweet_csvs(csv_dir):\n",
    "    pol_list = os.listdir(csv_dir)\n",
    "    full_df = pd.DataFrame()\n",
    "    for pol in pol_list:\n",
    "        temp_df = pd.read_csv(\"{}\\\\{}\".format(csv_dir, pol), lineterminator='\\n', index_col=0)\n",
    "        temp_df['UserID'] = pol.split(\"-\")[0]\n",
    "        full_df = pd.concat([temp_df, full_df])\n",
    "    full_df.to_csv('Data\\\\{}'.format('full_csv_test.csv'), index=False)\n",
    "\n",
    "\n",
    "combine_tweet_csvs(\"Data\\\\1000_only\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to add ground truth labels and the data will be ready for BERT.  The BERT tokenizer takes care of a lot of text processing jobs for us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(745000, 4)\n"
     ]
    }
   ],
   "source": [
    "def add_labels(pre_data_path):\n",
    "    pre_df = pd.read_csv(pre_data_path)\n",
    "    label_source = pd.read_csv('Data\\\\Dem_Rep_only.csv')\n",
    "    IDs = list(label_source['Account_ID'])\n",
    "    labels = list(label_source['Political_party'])\n",
    "    ID_label_dict = {IDs[i]: labels[i] for i in range(len(IDs))}\n",
    "    pre_df['Party'] = np.nan\n",
    "    for id in IDs:\n",
    "        # This try/except is only here so that the code will run\n",
    "        # without having filtered out unactive users and users\n",
    "        # with a bad ID, which would require connection to the\n",
    "        # twitter API.\n",
    "        try:\n",
    "            int_id = int(id)\n",
    "            pre_df.loc[pre_df[\"UserID\"] == int(id), 'Party'] = ID_label_dict[id]\n",
    "        except:\n",
    "            pass\n",
    "    print(pre_df.shape)\n",
    "    pre_df.to_csv('Data\\\\test_full_w_party.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "add_labels('Data\\\\full_csv_test.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19144ae99ecdd7de40b02483c8dd0c3f376ecc4a896bed5b8b3dc837c5c7c700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
