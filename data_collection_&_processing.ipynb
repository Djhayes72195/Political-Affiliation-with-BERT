{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "client = tweepy.Client('AAAAAAAAAAAAAAAAAAAAAAymjwEAAAAAu9hLQ%2Bd81apeChFZDERBToC3R%2F0%3DdfS4aRjydxJDCJPV7AsqTCIMUvpQHoqfhowAe7bzclCJoDkdXN')\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this notebook was used to mine and process the data that we used to train our BERT classifier. The starting point of our data collection is a dataset we found on Kaggle https://www.kaggle.com/datasets/mrmorj/us-politicians-twitter-dataset?resource=download. This dataset is the source of the twitter user IDs used for text data collection, as well as our ground truth labels. I have done my best to make this project as replicable as possible. For my own privacy, however, I have not included the Twitter API Bearer Token that would be required to interact with the Twitter API. As such, the code here can't be used to fetch more tweets or filter by account activity unless another Bearer token were provided. I will do my best to describe what happened during the gaps where the project is not replicable.  With the exception of these gaps, the code could be ran from top to bottom to reproduce the data collection and processing aspects of our project. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell defines and runs a function that filters our original Kaggle dataset down to two parties: Democrat and Republican. It saves the resulting dataset as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_third_party(pol_data: str):\n",
    "    \"\"\"This code sets party to \"Democrat\" (or \"Republican\") if \"Democrat\" (or \"Republican\") is contained in the name of the party in the original kaggle dataset.\"\"\"\n",
    "    pol_data = pd.read_csv(Path('Data/Original_kaggle_dataset.csv'))\n",
    "    df = pol_data[['Account_ID', 'Name', 'Twitter_username', 'Political_party']]\n",
    "    df = df.loc[df['Political_party'].str.contains('Republican') | df['Political_party'].str.contains('Democrat')]\n",
    "    df = df.drop_duplicates(subset=['Account_ID'])\n",
    "    df.loc[df['Political_party'].str.contains('Republican'), 'Political_party'] = 'Republican Party'\n",
    "    df.loc[df['Political_party'].str.contains('Democrat'), 'Political_party'] = 'Democratic Party'\n",
    "    df.to_csv(Path('Data/test_data/test_Dem_Rep_only.csv'), index=False)\n",
    "\n",
    "data = pd.read_csv(Path('Data/Original_kaggle_dataset.csv'))\n",
    "drop_third_party(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines a function that is used to filter out users who have not been active on twitter since October 1st, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_active_and_valid(user_id: str):\n",
    "    \"\"\"This functions returns False if the user has not been active since October 1 of 2022,\n",
    "    True otherwise.  It should also return false for bad (not numeric) IDs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        check = client.get_users_tweets(id=user_id,max_results=10, start_time='2022-10-01T00:00:00Z')\n",
    "    except:\n",
    "        return False\n",
    "    return (not check.data is None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell won't be run without a Bearer Token. Its purpose is to filter our data so as to only include accounts that have been active since Oct 1, 2022. As it turns out, it is quite common for a politician to have abandonded one or more twitter accounts. Maybe they forgot their password.  When we ran this function mid November of 2022 it filtered out nearly half of our rows, producing the dataset we called \"cleanest_politicians.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_data_cleaner_by_last_active(df):\n",
    "    \"\"\"This function will take a data frame representing twitter\n",
    "    users, and will take out anyone that has not been active\n",
    "    since Oct 1 2022.\n",
    "    \"\"\"\n",
    "    df = df.dropna(how='any',axis=0) \n",
    "    ID_list = list(df['Account_ID'])\n",
    "    ID_list_str = [str(x) for x in ID_list]\n",
    "    df['Active'] = [check_if_active_and_valid(x) for x in ID_list_str]\n",
    "    clean_df = df[df['Active'] == True]\n",
    "    clean_df = clean_df.drop(['Active'], axis=1)\n",
    "    return clean_df\n",
    "\n",
    "data = pd.read_csv(Path('Data/Dem_Rep_only.csv'))\n",
    "clean = pol_data_cleaner_by_last_active(data)\n",
    "clean.to_csv(Path('Data/cleanest_politicians.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the next cell to get 1000 tweets per Account ID. Initially, we sorted our data in a collection of csv files (one per politician).  It made it easier to manually inspect the data we were getting. The data for each politician is stored in this project under Data/politician_csvs/{ID}-{Name}, but we combined the data into one csv called \"text_party_IDs.csv\" for training/testing the model itself. \n",
    "\n",
    "Note: sometimes the Twitter API failed to get all 1000 tweets from a particular politician.  Those politicians were removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paged_tweets(user_id: int):\n",
    "    \"\"\"Function for gathering raw tweets for a list of user id and saving them to csv.\"\"\"\n",
    "    df = pd.DataFrame(columns=['TweetID', 'UserID', 'Text'])\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, exclude='retweets',\n",
    "                                  max_results=100).flatten(limit=1000):\n",
    "        # remove tab\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        #append raw tweet\n",
    "        df.loc[len(df.index)] = [tweet.id, user_id, text]\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(Path('Data/cleanest_politicians.csv'))\n",
    "ID_list = list(df['Account_ID'])\n",
    "name_list = list(df['Name'])\n",
    "\n",
    "   \n",
    "for i in range(0, len(ID_list)):\n",
    "    file = Path('Data/politician_csvs/{}-{}.csv'.format((ID_list[i]), name_list[i]))\n",
    "    if file.exists():\n",
    "        print(\"passed\")\n",
    "        pass\n",
    "    else:\n",
    "        print(file)\n",
    "        df = get_paged_tweets(ID_list[i])\n",
    "        df.to_csv(Path('Data/politician_csvs/{}-{}.csv'.format((ID_list[i]), name_list[i])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell filters out politicians that the Twitter API couldn't fetch all 1000 tweets for. All politicians that we could get 1000 tweets from are put in other folder: \"Data/1000_only\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pol_csvs_min_only(csv_path: str, min_tweets: int):\n",
    "    df = pd.read_csv(Path(\"Data/politician_csvs/{}\".format(csv_path)), lineterminator='\\n')\n",
    "    if len(list(df['TweetID'])) >= min_tweets:\n",
    "        file = Path(\"Data/1000_only/{}\".format(csv_path))\n",
    "        if file.exists():\n",
    "            pass\n",
    "        else:\n",
    "            os.rename(Path(\"Data/politician_csvs/{}\".format(csv_path)), Path(\"Data/1000_only/{}\".format(csv_path)))\n",
    "\n",
    "pol_csvs = os.listdir(Path(\"Data/politician_csvs\"))\n",
    "for i in range(0, len(pol_csvs)):\n",
    "    make_pol_csvs_min_only(pol_csvs[i], 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is only for testing make_pol_csvs_min_only. It moves all single politician CSVs back to \"Data/politician_csvs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_min_only():\n",
    "    csvs_to_move = os.listdir(Path(\"Data/1000_only\"))\n",
    "    for csv in csvs_to_move:\n",
    "        os.rename(Path(\"Data/1000_only/{}\".format(csv)), Path(\"Data/politician_csvs/{}\".format(csv)))\n",
    "    if len(os.listdir(Path(\"Data/1000_only\"))) == 0 and len(os.listdir(Path(\"Data/politician_csvs\"))):\n",
    "        print(\"All politican CSVs have been moved back.\")\n",
    "\n",
    "test_make_min_only()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we combine all Tweets into one csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tweet_csvs(csv_dir: str):\n",
    "    pol_list = os.listdir(csv_dir)\n",
    "    full_df = pd.DataFrame()\n",
    "    for pol in pol_list:\n",
    "        temp_df = pd.read_csv(Path(\"{}/{}\".format(csv_dir, pol)), lineterminator='\\n', index_col=0)\n",
    "        temp_df['UserID'] = pol.split(\"-\")[0]\n",
    "        full_df = pd.concat([temp_df, full_df])\n",
    "    full_df.to_csv(Path('Data/test_data/{}'.format('test_text_party_IDs.csv')), index=False)\n",
    "\n",
    "\n",
    "combine_tweet_csvs(Path(\"Data/1000_only\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to add ground truth labels and the data will be ready to pass to classifytweets.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(pre_data_path: str):\n",
    "    pre_df = pd.read_csv(pre_data_path)\n",
    "    label_source = pd.read_csv(Path('Data/test_data/test_Dem_Rep_only.csv'))\n",
    "    IDs = list(label_source['Account_ID'])\n",
    "    labels = list(label_source['Political_party'])\n",
    "    ID_label_dict = {IDs[i]: labels[i] for i in range(len(IDs))}\n",
    "    pre_df['Party'] = np.nan\n",
    "    for id in IDs:\n",
    "        # This try/except is only here so that the code will run\n",
    "        # without having filtered out unactive users and users\n",
    "        # with a bad ID, which would require connection to the\n",
    "        # twitter API.\n",
    "        try:\n",
    "            int_id = int(id)\n",
    "            pre_df.loc[pre_df[\"UserID\"] == int(id), 'Party'] = ID_label_dict[id]\n",
    "        except:\n",
    "            pass\n",
    "    # (745000, 4) should be printed.\n",
    "    print(pre_df.shape)\n",
    "    pre_df.to_csv(Path('Data/test_data/test_text_party_IDs.csv'), index=False)\n",
    "\n",
    "add_labels(Path('Data/test_data/test_text_party_IDs.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 TweetID      UserID  \\\n",
      "0    1636438890088153093  1623308912   \n",
      "1    1636376701218459656  1623308912   \n",
      "2    1636177603194232838  1623308912   \n",
      "3    1636177599427670016  1623308912   \n",
      "4    1636054595737714689  1623308912   \n",
      "..                   ...         ...   \n",
      "995  1496238085876686856  1623308912   \n",
      "996  1496205450739658752  1623308912   \n",
      "997  1496128683635052555  1623308912   \n",
      "998  1495932970129141766  1623308912   \n",
      "999  1495880217549844480  1623308912   \n",
      "\n",
      "                                                  Text  \n",
      "0    We sat down with the Ecuadorian Federation of ...  \n",
      "1    Dealing with the federal bureaucracy can often...  \n",
      "2    Read my full statement here: https://t.co/OkMr...  \n",
      "3    The delegation met today with Ecuadorian presi...  \n",
      "4    We had a productive conversation with Minister...  \n",
      "..                                                 ...  \n",
      "995  Why do illegal immigrants keep pouring across ...  \n",
      "996  Chuck Todd asked a great question this Sunday....  \n",
      "997  Biden wants to use a permanent pandemic narrat...  \n",
      "998  Missouri students at Howell Valley School cele...  \n",
      "999  Happy President’s Day!   Today, we honor the p...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test if we can get new tweets\n",
    "def get_paged_tweets(user_id: int):\n",
    "    \"\"\"Function for gathering raw tweets for a list of user id and saving them to csv.\"\"\"\n",
    "    df = pd.DataFrame(columns=['TweetID', 'UserID', 'Text'])\n",
    "    for tweet in tweepy.Paginator(client.get_users_tweets, user_id, exclude='retweets',\n",
    "                                  max_results=100).flatten(limit=1000):\n",
    "        # remove tab\n",
    "        text = tweet.text.replace('\\n',' ')\n",
    "        #append raw tweet\n",
    "        df.loc[len(df.index)] = [tweet.id, user_id, text]\n",
    "    return df\n",
    "\n",
    "print(get_paged_tweets(1623308912))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83887cf659fc38d41113aac5f04d1cc1a1d52606a37b6827b963e0b6a56a971c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
