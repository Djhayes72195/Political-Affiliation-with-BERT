{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to evalute the results from the BERT model. At the moment, the code is written to consider the results from a run conducted on 1/29/2023  (Data\\Raw_BERT_results\\testresults_1_29_23.csv).  This first cell takes the csv results file that was written by classifytweets.py, replaces the party with the proper coding {Democrat: 0, Republican: 1}, and writes the coded data in a csv for use by the second cell in this notebook. It also reports chunk-wise accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_wise_accuracy(BERT_results, output_file_name):\n",
    "    BERT_results = BERT_results.drop(['Text'], axis=1)\n",
    "    print(BERT_results)\n",
    "    BERT_results.loc[BERT_results['Party'] == \"Democratic Party\", \"Party\"] = 0 \n",
    "    BERT_results.loc[BERT_results['Party'] == \"Republican Party\", \"Party\"] = 1\n",
    "    new_cols = [\"UserID\", \"Party\", \"Predictions\"]\n",
    "    BERT_results = BERT_results[new_cols]\n",
    "    pred_arr = BERT_results[\"Predictions\"].to_numpy()\n",
    "    truth_arr = BERT_results[\"Party\"].to_numpy()\n",
    "    accuracy = np.sum(pred_arr == truth_arr)/len(pred_arr)\n",
    "    BERT_results.to_csv(Path(\"Data/Data_for_Evaluation/{}.csv\".format(output_file_name)), index=False)\n",
    "    return \"Chunk level accuracy is {}\".format(accuracy)\n",
    "\n",
    "# change the path in the next line by replacing the last 1 with a 2 to test the results from a different run.\n",
    "results = pd.read_csv(Path('Data/Raw_BERT_results/testresults_1_29_23.csv'), index_col=0)\n",
    "print(get_chunk_wise_accuracy(results, \"testresults_1_29_23_chunk\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_accuracy(chunk_results, output_file_name):\n",
    "    user_list = list(set(chunk_results[\"UserID\"]))\n",
    "    correct_pred = 0\n",
    "    user_pred = pd.DataFrame(columns = [\"UserID\", \"Party\", \"Predictions\"])\n",
    "    for user in user_list:\n",
    "        user_df = chunk_results.loc[chunk_results[\"UserID\"] == user]\n",
    "        party = int(user_df.mode()[\"Party\"])\n",
    "        prediction = int(user_df.mode()[\"Predictions\"])\n",
    "        user_pred = user_pred.append({\"UserID\": user, \"Party\": party, \"Predictions\": prediction}, ignore_index=True)\n",
    "        if party == prediction:\n",
    "            correct_pred += 1\n",
    "    accuracy = correct_pred/len(user_list)\n",
    "    user_pred.to_csv(Path(\"Data/Data_for_evaluation/{}.csv\".format(output_file_name)), index=False)\n",
    "    return \"User level accuracy is: {0:.3f}\".format(accuracy)\n",
    "\n",
    "chunk_results = pd.read_csv(Path(\"Data/Data_for_evaluation/testresults_1_29_23_chunk.csv\"))\n",
    "print(get_user_accuracy(chunk_results, \"testresults_1_29_23_user\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will calculate the confusion matrix for either user-level or chunk-level results.  I consider Democrat -> Positive, Republican -> Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(results):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    predictions = list(results[\"Predictions\"])\n",
    "    labels = list(results[\"Party\"])\n",
    "    for i in range(0, len(labels)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            if labels[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if labels[i] == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN}\n",
    "\n",
    "user_results = pd.read_csv(Path(\"Data/Data_for_evaluation/testresults_1_29_23_user.csv\"))\n",
    "print(get_confusion_matrix(user_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19144ae99ecdd7de40b02483c8dd0c3f376ecc4a896bed5b8b3dc837c5c7c700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
