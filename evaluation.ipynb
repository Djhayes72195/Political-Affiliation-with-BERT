{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to evalute the results from the BERT model.  At the moment, the code is written to consider the results from \"RUN1\" (Data\\Raw_BERT_results\\sample_chunk_results1.csv).  I have included the raw results from another run, called \"RUN2\" at Data\\Raw_BERT_results\\sample_chunk_results2.csv.  Should you so desire, you could run everything for RUN2 by changing a couple paths/names.  This first cell takes the csv file that was written by classifytweets.py, replaces the party with the proper coding {Democrat: 0, Republican: 0}, writes the coded data in a csv for use by the second cell in this notebook. It also reports chunk-wise accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk level accuracy is 0.8\n"
     ]
    }
   ],
   "source": [
    "def get_chunk_wise_accuracy(BERT_results, output_file_name):\n",
    "    BERT_results = BERT_results.drop(['Text'], axis=1)\n",
    "    BERT_results.loc[BERT_results['Party'] == \"Democratic Party\", \"Party\"] = 0 \n",
    "    BERT_results.loc[BERT_results['Party'] == \"Republican Party\", \"Party\"] = 1\n",
    "    new_cols = [\"UserID\", \"Party\", \"Predictions\"]\n",
    "    BERT_results = BERT_results[new_cols]\n",
    "    pred_arr = BERT_results[\"Predictions\"].to_numpy()\n",
    "    truth_arr = BERT_results[\"Party\"].to_numpy()\n",
    "    accuracy = np.sum(pred_arr == truth_arr)/len(pred_arr)\n",
    "    BERT_results.to_csv(\"Data\\\\Data_for_Evaluation\\\\{}_chunk_accuracy={}.csv\".format(output_file_name, accuracy), index=False)\n",
    "    return \"Chunk level accuracy is {}\".format(accuracy)\n",
    "\n",
    "# change the path in the next line by replacing the last 1 with a 2 to test the results from a different run.\n",
    "results = pd.read_csv('Data\\\\Raw_BERT_results\\\\sample_chunk_results1.csv', index_col=0)\n",
    "print(get_chunk_wise_accuracy(results, \"RUN1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User level accuracy is: 0.858\n"
     ]
    }
   ],
   "source": [
    "def get_user_accuracy(chunk_results, output_file_name):\n",
    "    user_list = list(set(chunk_results[\"UserID\"]))\n",
    "    correct_pred = 0\n",
    "    user_pred = pd.DataFrame(columns = [\"UserID\", \"Party\", \"Predictions\"])\n",
    "    for user in user_list:\n",
    "        user_df = chunk_results.loc[chunk_results[\"UserID\"] == user]\n",
    "        party = int(user_df.mode()[\"Party\"])\n",
    "        prediction = int(user_df.mode()[\"Predictions\"])\n",
    "        user_pred = user_pred.append({\"UserID\": user, \"Party\": party, \"Predictions\": prediction}, ignore_index=True)\n",
    "        if party == prediction:\n",
    "            correct_pred += 1\n",
    "    accuracy = correct_pred/len(user_list)\n",
    "    user_pred.to_csv(\"Data\\\\Data_for_evaluation\\\\{}_user_accuracy={}.csv\".format(output_file_name, accuracy))\n",
    "    return \"User level accuracy is: {0:.3f}\".format(accuracy)\n",
    "\n",
    "chunk_results = pd.read_csv(\"Data\\\\Data_for_evaluation\\\\RUN1_chunk_accuracy=0.8.csv\")\n",
    "print(get_user_accuracy(chunk_results, \"RUN1\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will calculate the confusion matrix for either user-level or chunk-level results.  I consider Democrat -> Positive, Republican -> Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TP': 77, 'FP': 13, 'FN': 14, 'TN': 86}\n"
     ]
    }
   ],
   "source": [
    "def get_confusion_matrix(results):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    predictions = list(results[\"Predictions\"])\n",
    "    labels = list(results[\"Party\"])\n",
    "    for i in range(0, len(labels)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            if labels[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if labels[i] == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    return {\"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN}\n",
    "\n",
    "user_results = pd.read_csv(\"Data\\Data_for_evaluation\\RUN1_user_accuracy=0.8578947368421053.csv\")\n",
    "print(get_confusion_matrix(user_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19144ae99ecdd7de40b02483c8dd0c3f376ecc4a896bed5b8b3dc837c5c7c700"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
